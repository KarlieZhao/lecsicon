<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <title>Lecsicon - about</title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Special+Elite&display=swap');
    </style>
    <style>
        body {
            font-size: 20px;
            color: black;
            font-family: 'Special Elite', cursive;
            background-color: #EFF0F1;
        }
        
        .textarea {
            display: block;
            margin: auto;
            margin-top: 10%;
            margin-bottom: 10%;
            word-break: break-word;
            border: 0;
            font-family: inherit;
            font-size: 1.8vmin;
            letter-spacing: 0.03em;
            word-spacing: 0.3em;
            line-height: 35px;
            display: block;
            width: 55%;
            overflow: auto;
        }
        
        h1 {
            font-size: 50px;
        }
        
        h2 {
            font-size: 30px;
        }
        
        .title {
            position: absolute;
            top: 130px;
            left: 50%;
            line-height: 2.4rem;
            width: 45%;
        }
        
        a {
            color: black;
            text-decoration: underline;
            text-decoration-thickness: 1.5px;
        }
        
        .crossout {
            text-decoration: line-through;
        }
        
        hr {
            border: 1px dashed black;
        }
        
        .footnote {
            text-align: center;
            font-size: small;
        }
    </style>
</head>

<body>

    <div class="textarea">
        <h2><a href="./index.html" style="color:#880000">LECSICON</a></h2>

        <span style="color:#880000">L</span>inguists <span style="color:#880000">e</span>nthusiastically <span style="color:#880000">c</span>atalog <span style="color:#880000">s</span>ymbols,
        <span style="color:#880000">i</span>nterpreting <span style="color:#880000">c</span>arefully
        <span style="color:#880000">o</span>ccurred
        <span style="color:#880000">n</span>uances.

        <br><br><br>"I will give you a word. Make a sentence or phrase with a series of words whose first letters sequentially spell out my word. Your sentence doesn't have to have a strong semantic connection with the word I give you.
        <br><br> Here're some good examples:
        <br> Cake - Creating amazing kitchen experiences.
        <br> Fire - Fierce inferno razed everything.
        <br>Smile - Some memories invoke lovely emotions.
        <br><br> Here's a bad example:
        <br> Abandon - Alice abandoned <span class="crossout">her plans to move to the city when she realized the cost of living was too high.</span>
        <br><br> Now make a sentence for /WORD/. <br><br> According to our rules, there should be /N/ words in your sentence. Your response should only contain the sentence you make. "
        <br><br>
        <hr>
        <br><br> Above is a prompt I wrote for the large language model GPT 3.5 Turbo which currently powers chatGPT. In response to my prompt, the model creates <a href="https://www.poetryfoundation.org/search?query=Acrostic" target="_blank">acrostics</a>        but in sentences. Thousands of word-sentence pairs compose this project, Lecsicon: Linguists Enthusiastically Catalog Symbols, Interpreting Carefully Occurred Nuances.

        <br><br>The accuracy of GPT's responses, as usual, was not guaranteed. At times, one or two extra words slip into the sentence. In extreme cases, the model rambled on and lost track of the initial instructions. Out of over 27,000 attempts, 7828
        results strictly followed my rules. When one visits the Lecsicon web page, the 7828 word-sentence pairs are typed out letter-by-letter by a program. Beginning with a random word, the program scans the Lecsicon database, and strives to find a new
        word that has the smallest Levenshtein distance from the preceding word. The temperature slider on the web page adjusts the scope of the program, yielding either stronger or weaker connections between consecutive words displayed. Typing speed
        is also influenced by temperature - in the same way as molecular motion is.
        <br><br> Language games like acrostics captivate people like me because there is a hidden layer behind putting words together following certain mechanisms. Making sense--the resulting sentences provide a greater context for the original words
        and reveal new perspectives. It's not a coincidence that GPT makes sentences that relate to the original word's meaning. Rather, it's a deliberate calculation and balancing process to capture words based on their relations to each other: a ghost
        wandering through a latent word vector space. Any word in such a space is nothing but the linguistic associations around it, based on statistical computation of the patterns of the text huamans have produced. Sentences emerge from their context,
        and perhaps that's why some entries are intriguing, such as "Music: Many unexpected sounds indicate creativity", or "Date: Dinner and theater experience". Lecsicon is only made possible by the fractal nature of the English language: the way we
        make sense of letters, words, sentences, and paragraphs.

        <br><br> <br>
        <!-- <h3>Syntax and Semantics</h3> -->
        <!-- Language games such as acrostics or anagrams certainly have an allure that captivates people like me, because there is a hidden layer behind the simple switch and swap of letters. Making sense--in some instances of anagram, from 'listen' to 'silent',
        from 'gentleman' to 'elegant man', the resulting words convey relevant meanings to the original ones and reveal new perspectives. Making sense also differentiate letters from random symbols or doodles, and this communal experience to make sense
        of symbols is what we call as a language. Lecsicon is only made possible by the fractal feature of the English language: letters, words, sentences, paragraphs, passages. Acrostics play with letters and words as well, but on a slightly larger scale. -->

        <!-- <br><br>Inspired by experimental writers such as the <a href="https://poets.org/text/brief-guide-oulipo" target="_blank">Oulipo</a> group and their works, more specifically, <a href="https://en.wikipedia.org/wiki/Alphabetical_Africa" target="_blank">Alphabetical Africa</a>        by Walter Abish and <a href="https://dannyreviews.com/h/A_Void.html" target="_blank">A Void</a> by Georges Perec, this is an attempt of constrained writing in collaboration with GPT 3.5 Turbo, a machine learning language model. From a number of
        entries, what we can see in Lecsicon is the model's efforts of trying to tie the sentence back to the meaning of the original word, although my prompt has clearly stated that the output sentence "doesn't need to have a strong semantic connection
        with the word".

        <br><br> And it's not a coincidence that the model made sentences that are relevant to the original word. Closely examining the failed attempts, one can almost tell the model's struggles trying to fulfill the rules while making the sentence grammatically
        correct and semantically coherent. It is a deliberate calculation and balancing process of the model to capture words and put them into a sentence, based on the words' relations to each other. And perhaps that's why we find a lot of entries intriguing,
        such as "Music: Many unexpected sounds indicate creativity", "Date: Dinner and theater experience", or "Language: Learning another never gives us a greater experience" (damn, this is so good). -->
        <!-- <br><br> Dictionaries are human inventions to define words with other words. Now, in large language models, the idea of “definition” dissolves, and is replaced by words in the form of vectors mapped into high-dimensional spaces. Any
        word in such a space is nothing but the linguistic associations around it, based on statistical computation of the patterns of the text we produce. <br><br> -->
        <!-- <br><br>Lecsicon is an attempt to carve a crack in the wall of linguistic forms and language models so that we can peek into them. The output might come out hilarious or playful, and the intention of creating this piece was never serious. -->
        <!-- <img src="./images/animalspace1.png" style="width: 25vw">
        <img src="./images/animalspace2.png" style="width: 25vw"> -->
        <!-- <div class="footnote"> <i>Mapping word vectors. Images from <a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469" target="_blank">Understanding Word Vectors</a>, a great Jupyter notebook by <a href="https://www.decontextualize.com/" target="_blank">Allison Parrish</a></i></div> -->
        <!-- <br><br> Constrained writing is particularly interesting with computers because they are the best to execute strict, clear binary rules. I'm going to quote Charles Hartman in <a href="https://muse.jhu.edu/book/2399/" target="_blank">Virtual Muse: Experiments in Computer Poetry</a>:
        Language models offer childish pleasures…the wickedness of exploding revered literary scripture into babble. […] Here is language creating itself out of nothing, out of mere statistical noise. As we raise N, we can watch sense evolve and meaning
        stagger up onto its own miraculous feet…(He is talking in the context of the Markov chains.) -->
        <!-- <br><br> GPT 3.5 only succeeded in one fourth of the tasks. It surely will get better with GPT 4. Now that we have a tool that can create non-exhaustive textual content that we can spend our lifetime reading, what do we do with it? Pioneers in
        computational writing has come to this realization long ago--in his 2011 book <a href="https://www.jstor.org/stable/10.7312/gold14990" target="_blank">Uncreative Writing: Managing Language in the Digital Age</a>, Kenneth Goldsmith bluntly nailed
        the issue down at the very beginning: “‘The world is full of texts, more or less interesting; I do not wish to add any more.’ It seems an appropriate response to a new condition in writing today: faced with an unprecedented amount of available
        text, the problem is not needing to write more of it; instead, we must learn to negotiate the vast quantity that exists. ” If language can be seen as a material, text should be not only produced, but also manipulated, repurposed, patchwritten,
        and sampled.

        <br><br> Lecsicon is an attempt to dig into the form of language and the motivation of large language models. The output might come out hilarious or playful, and the intention of creating this piece was never serious. Have fun reading! -->
        
        <hr>
        <br><br><br>Side Notes:
        <br><br> 1. Holes on the Map
        <br> What might also be worth mentioning is the nonword lecsicon that has not been published. As its name suggests, nonword lecsicon is a list of word-sentence pairs with made-up words that were created based on lexical features of English, sourced
        from <a href="https://elexicon.wustl.edu/" target="_blank">The English Lexicon Project</a> dataset. The GPT model did a fairly good job in this task:
        <br><br> Athorred - Ants tirelessly hauling objects, relentlessly roaming every day.
        <br> Achorrent - Alligators can hold oranges, reaching rare edible nuts tenaciously.
        <br> Avides - Artistic visionaries inspire diverse, expressive scenes.
        <br> Arides - Adventurous raccoons investigate deep, elusive spaces.
        <br> Ahiding - Agile hares idly dart in narrow gardens.
        <br> Abigoul - Artful butterflies indulge gracefully on unique leaves.
        <br> Afility - Ancient frogs inhabit lily pads intranquil yards.
        <br> ...
        <br><br> Yeah, GPT can generate sentences that perfectly satisfy the rules for words that don't exist, which is really interesting and worth pondering...
    </div>
</body>

</html>