<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <title>Lecsicon - about</title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Special+Elite&display=swap');
    </style>
    <style>
        body {
            font-size: 20px;
            color: black;
            font-family: 'Special Elite', cursive;
            background-color: #EFF0F1;
        }
        
        .textarea {
            display: block;
            margin: auto;
            margin-top: 10%;
            margin-bottom: 10%;
            word-break: break-word;
            border: 0;
            font-family: inherit;
            font-size: inherit;
            letter-spacing: 0.03em;
            word-spacing: 0.3em;
            line-height: 35px;
            display: block;
            overflow: hidden;
            width: 55%;
            overflow: scroll;
        }
        
        h1 {
            font-size: 50px;
        }
        
        h2 {
            font-size: 30px;
        }
        
        .title {
            position: absolute;
            top: 130px;
            left: 50%;
            line-height: 2.4rem;
            width: 45%;
        }
        
        a {
            color: black;
            text-decoration: underline;
            text-decoration-thickness: 1.5px;
        }
        
        .crossout {
            text-decoration: line-through;
        }
        
        hr {
            border: 1px dashed black;
        }
        
        .footnote {
            text-align: center;
            font-size: small;
        }
    </style>
</head>

<body>

    <div class="textarea">
        <h2><a href="./index.html">LECSICON</a></h2>
        <br><br>"I will give you a word. Make a sentence with a series of words whose first letters sequentially compose my word. Your sentence doesn't have to have a strong semantic connection with the word I give you.
        <br><br> Here're some good examples:
        <br> Cake - Creating amazing kitchen experiences.
        <br> Fire - Fierce inferno razed everything.
        <br>Smile - Some memories invoke lovely emotions.
        <br><br> Here's a bad example:
        <br> Abandon - Alice abandoned <span class="crossout">her plans to move to the city when she realized the cost of living was too high.</span>
        <br><br> Now make a sentence for /WORD/. <br><br> According to our rules, there should be /N/ words in your sentence. Your response should only contain the sentence you make. "
        <br><br>
        <hr>
        <br><br> Above is a prompt I wrote for the large language model GPT 3.5 Turbo--one may have been the most advanced language model, but its position will soon be replaced by its descendant, GPT 4--who responded to me with a sentence for each word
        I gave it. As per my instructions, GPT should produce sentences in a way that results in the combination of the initial letter of each word composing my original words. Thousands of word-sentence pairs like this eventually formed this project--<i><b>Lecsicon: Linguists Enthusiastically Catalog Symbols, Interpreting Carefully Occurred Nuances</b></i>,
        both in the format of a website and a physical book.
        <br><br>The name of the project implies its mechanism: an <a href="https://en.wikipedia.org/wiki/Acrostic" target="_blank">acrostic</a>. However, instead of producing a poem, the model is tasked with generating a single sentence for each word,
        hence the room for creativity is largely restricted by the specific number of words. The language model, once again, beated humans in its efficiency. As I sent it an array of words along with the prompt, it responded to me in seconds with
        a list of sentences. The accuracy, however, was not guaranteed. Sometimes there are one or two extra words in the sentence, or in extreme cases the language model just went on and on in the hope to make the sentence complete, creating
        sentences like "in moments of adversity, great minds always conjure intricate tales, inspiring new and unique, ambitious notions" for the word "imagination". Upon receiving the responses, I use a snippet of code to split them into a 'correct'
        group and an 'incorrect' group. Out of 4830 fetches, I got 882 results that strictly followed the rules.

        <br><br> <br>
        <h3>Syntax and Semantics</h3>
        Language games such as acrostics or anagrams certainly have an allure that captivates people like me, because there is a hidden layer behind the simple switch and swap of letters. Making sense--in some instances of anagram, from 'listen' to 'silent',
        from 'gentleman' to 'elegant man', the resulting words convey relevant meanings to the original ones and reveal new perspectives. Making sense also differentiate letters from random symbols or doodles, and this communal experience to make sense
        of symbols is what we call as a language. Lecsicon is only made possible by the fractal feature of the English language: letters, words, sentences, paragraphs, passages. Acrostics play with letters and words as well, but on a slightly larger scale.

        <br><br>Inspired by experimental writers such as the <a href="https://en.wikipedia.org/wiki/Oulipo#:~:text=Oulipo%20(French%20pronunciation%3A%20%E2%80%8B%5B,works%20using%20constrained%20writing%20techniques." target="_blank">Oulipo</a> group
        and their works, more specifically, <a href="https://en.wikipedia.org/wiki/Alphabetical_Africa" target="_blank">Alphabetical Africa</a> by Walter Abish and <a href="https://en.wikipedia.org/wiki/A_Void" target="_blank">A Void</a> by Georges Perec,
        this is an attempt of constrained writing in collaboration with GPT 3.5 Turbo, a machine learning language model. From a number of entries, what we can see in Lecsicon is the model's efforts of trying to tie the sentence back to the meaning of the original
        word, although my prompt has clearly stated that the output sentence "doesn't need to have a strong semantic connection with the word".

        <br><br> And it's not a coincidence that the model made sentences that are relevant to the original word. Closely examining the failed attempts, one can almost tell the model's struggles trying to fulfill the rules while making the sentence grammatically
        correct and semantically coherent. It is a deliberate calculation and balancing process of the model to capture words and put them into a sentence, based on the words' relations to each other. And perhaps that's why we find a lot of entries intriguing,
        such as "Music: Many unexpected sounds indicate creativity", "Date: Dinner and theater experience", or "Language: Learning another never gives us a greater experience" (damn, this is so good).

        <br> <br><br>
        <h3>Definitions and Relationality</h3>
        Dictionaries are human inventions that “lists the words of a language and gives their meanings” (this definition is drawn from the Oxford Languages dictionary). What we do in dictionaries is to use other words to elucidate a given word--a definition of
        a word only exists in relation to other words that define it. Now, in large language models, the idea of “definition” or “meaning' dissolves, and is replaced by words in the form of vectors mapped into high dimensional spaces. Any word in such
        a space is nothing but the linguistic association with it, based on statistical computation of the patterns of the text we produced.
        <br><br>
        <img src="./images/wordEmbeddings2.png" style="width: 25vw">
        <img src="./images/wordEmbeddings3d.png" style="width: 25vw"><br>
        <div class="footnote"> <i>2D and 3D visualizations of word2vec embeddings. <a href="https://www.mathworks.com/help/textanalytics/ug/visualize-word-embedding-using-text-scatter-plot.html" target="_blank">[source]</a></i>
        </div><br><br>
        <img src="./images/animalspace1.png" style="width: 25vw">
        <img src="./images/animalspace2.png" style="width: 25vw">
        <div class="footnote"> <i>Mapping word vectors. Images from <a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469" target="_blank">Understanding Word Vectors</a>, a great Jupyter notebook by <a href="https://www.decontextualize.com/" target="_blank">Allison Parrish</a></i></div>
        <br><br> Constrained writing is particularly interesting with computers because they are the best to execute strict, clear binary rules. I'm going to quote Charles Hartman in <a href="https://muse.jhu.edu/book/2399/" target="_blank">Virtual Muse: Experiments in Computer Poetry</a>:
        Language models offer childish pleasures…the wickedness of exploding revered literary scripture into babble. […] Here is language creating itself out of nothing, out of mere statistical noise. As we raise N, we can watch sense evolve and meaning
        stagger up onto its own miraculous feet…(He is talking in the context of the Markov chains.)
        <br><br> GPT 3.5 only succeeded in one fifth of the tasks. It surely will get better with GPT 4. Now that we have a tool that can create non-exhaustive textual content that we can spend our lifetime reading, what do we do with it? Pioneers in
        computational writing has come to this realization long ago--in his 2011 book <a href="https://www.jstor.org/stable/10.7312/gold14990" target="_blank">Uncreative Writing: Managing Language in the Digital Age</a>, Kenneth Goldsmith bluntly nailed
        the issue down at the very beginning: “‘The world is full of texts, more or less interesting; I do not wish to add any more.’ It seems an appropriate response to a new condition in writing today: faced with an unprecedented amount of available
        text, the problem is not needing to write more of it; instead, we must learn to negotiate the vast quantity that exists. ” If language can be seen as a material, text should be not only produced, but also manipulated, repurposed, patchwritten,
        and sampled.

        <br><br> Lecsicon is an attempt to dig into the form of language and the motivation of large language models. The output might come out hilarious or playful, and the intention of creating this piece was never serious. Have fun reading!

        <br><br><br><br> Notes:
        <br><br> 1. Holes on the Map
        <br> What might also be worth mentioning is the nonword lecsicon that has not been published. As its name suggests, nonword lecsicon is a list of word-sentence pairs with made-up words that were created based on lexical features of English, sourced
        from <a href="https://elexicon.wustl.edu/" target="_blank">The English Lexicon Project</a> dataset. The GPT model did a fairly good job is this task:
        <br><br> Athorred - Ants tirelessly hauling objects, relentlessly roaming every day.
        <br> Achorrent - Alligators can hold oranges, reaching rare edible nuts tenaciously.
        <br> Avides - Artistic visionaries inspire diverse, expressive scenes.
        <br> Arides - Adventurous raccoons investigate deep, elusive spaces.
        <br> Ahiding - Agile hares idly dart in narrow gardens.
        <br> Abigoul - Artful butterflies indulge gracefully on unique leaves.
        <br> Afility - Ancient frogs inhabit lily pads intranquil yards.
        <br> ...
        <br><br> Isn't it compelling and worth pondering, the reason why GPT can generate sentences that perfectly satisfy the rules for non-existing words?

        <br><br>2. On the index page of the website, an image generated by Midjourney is displayed along with the list of entries. The image is a blend of <a href="https://cdn.discordapp.com/ephemeral-attachments/1062880104792997970/1086473155742269470/0232907e2cd5c6353be0d2584b18b1cb.jpeg"
            target="_blank">this</a>, <a href="https://cdn.discordapp.com/ephemeral-attachments/1062880104792997970/1086473158002999326/f08ba731e397dc3d4c3eaa938f340561.jpeg" target="_blank">this</a>, and <a href="https://cdn.discordapp.com/ephemeral-attachments/1062880104792997970/1086473157042520174/WordEmbeddingstSNE.png"
            target="_blank">this</a>, drawing ideas from visualizations of word2vec embeddings.
        <br><br>3. The output from non-words will be published soon, stay tuned!

    </div>
</body>

</html>